{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4bd7fe-5b9b-448b-9535-d566136ca2d9",
   "metadata": {},
   "source": [
    "# Generates FMNIST-like stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28421fd-d100-4c20-bbbd-47b43d18c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms.v2 as tv2\n",
    "import torchvision.datasets as tds\n",
    "import torchvision.utils as tu\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bfc276-cb98-4b72-b751-66b37deae82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"./\"\n",
    "cpu_num = 4\n",
    "\n",
    "tfs = tv2.Compose([\n",
    "    tv2.ToImage(),\n",
    "    tv2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "mnist_train = tds.FashionMNIST(\n",
    "    root=dataset_root,\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=tfs\n",
    ")\n",
    "\n",
    "mnist_eval = tds.FashionMNIST(\n",
    "    root=dataset_root,\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=tfs\n",
    ")\n",
    "\n",
    "\n",
    "batchsize = 32\n",
    "mnist_train = tds.wrap_dataset_for_transforms_v2(mnist_train)\n",
    "mnist_eval = tds.wrap_dataset_for_transforms_v2(mnist_eval)\n",
    "\n",
    "train_loader = tud.DataLoader(mnist_train, batch_size=batchsize, num_workers=cpu_num, shuffle=True)\n",
    "val_loader = tud.DataLoader(mnist_eval, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4037976b-6e3b-42d2-8a8f-18396ee75dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "C=64\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.conv0 = nn.Conv2d(\n",
    "            channels, C,\n",
    "            stride = 2,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode='replicate'\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            C, C,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode='replicate'\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            C, C * 2,\n",
    "            stride=2,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode='replicate'\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            C*2, C*2,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode='replicate'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.conv0 = nn.ConvTranspose2d(\n",
    "            channels, C*2,\n",
    "            stride=2,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            output_padding=1\n",
    "        )\n",
    "        self.conv1 = nn.ConvTranspose2d(\n",
    "            C*2, C*2,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv2 = nn.ConvTranspose2d(\n",
    "            C*2, C,\n",
    "            stride=2,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            output_padding=1\n",
    "        )\n",
    "        self.conv3 = nn.ConvTranspose2d(\n",
    "            C, 1,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "# channels - channels in input and output image\n",
    "# dims - spatial dimensions of image (28x28 for mnist)\n",
    "# hiddensz - latent vector size\n",
    "# params - number of f32 params to condition on.\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, channels, dim, hiddensz, latentsz, param_num):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(channels + param_num)\n",
    "        self.dim=dim\n",
    "        self.hw = dim // 4\n",
    "        \n",
    "        self.encoder_fc = nn.Linear(C * 2 * self.hw * self.hw, hiddensz)\n",
    "        self.mu_fc = nn.Linear(hiddensz, latentsz)\n",
    "        self.logvar_fc = nn.Linear(hiddensz, latentsz)\n",
    "\n",
    "        self.decoder_fc = nn.Linear(param_num + latentsz, C * 2 * self.hw * self.hw)\n",
    "\n",
    "        self.decoder = Decoder(C * 2)\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, params):\n",
    "        # Cat the parameters onto the input as full input channels\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        # params (b, param_num)\n",
    "        param_channels = params[..., None, None].expand(-1, -1, self.dim, self.dim)\n",
    "        x = torch.cat((param_channels, x), dim=1)\n",
    "        # -> (64, 7, 7)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = self.encoder_fc(x.flatten(start_dim=1))\n",
    "        mu = self.mu_fc(x)\n",
    "        logvar = self.logvar_fc(x)\n",
    "\n",
    "        latent = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Cat the parameters onto the latent vector\n",
    "        z = torch.cat((params, latent), dim=1)\n",
    "        z = self.decoder_fc(z).reshape(-1, C * 2, self.hw, self.hw)\n",
    "        \n",
    "        z = self.decoder(z)\n",
    "        \n",
    "        return z, latent\n",
    "\n",
    "    def sample(self, params, latent):\n",
    "        z = torch.cat((params, latent), dim=1)\n",
    "        z = self.decoder_fc(z).reshape(-1, C * 2, self.hw, self.hw)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43a78a-bedd-4ebc-a95a-1368ba97b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device='cuda'\n",
    "\n",
    "# model = MyNn(28 * 28, 10, hidden_sz=32).to(device)\n",
    "model = CVAE(\n",
    "    channels=1,\n",
    "    dim=28,\n",
    "    hiddensz=512,\n",
    "    latentsz=256,\n",
    "    param_num=1)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
    "lossfn = nn.MSELoss()\n",
    "epochs = 50\n",
    "\n",
    "loss_plot = []\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        images = images.float().to(device)\n",
    "\n",
    "        # Unsqueeze here so that params is (b, param_num), but param_num=1\n",
    "        params = target.unsqueeze(-1).to(device)\n",
    "\n",
    "        outs, _ = model(images, params)\n",
    "        loss = lossfn(outs, images)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    tmp = torch.tensor(losses[-len(train_loader):]).mean().item()\n",
    "    print(tmp)\n",
    "    loss_plot.append(tmp)\n",
    "\n",
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622692d-ec39-460b-aed9-54fb36f48cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(p=(0.0, 1.0))\n",
    "def plot(p=0.0):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        data, lbl = mnist_train[1000]\n",
    "        print(data.shape, lbl)\n",
    "        s, latent0 = model(data.unsqueeze(0).to(device), torch.tensor([lbl]).unsqueeze(0).to(device))\n",
    "        d0 = torch.cat((data.squeeze().cpu(), s.squeeze().cpu()), dim=1)\n",
    "    \n",
    "        data, lbl = mnist_train[1]\n",
    "        s, latent1 = model(data.unsqueeze(0).to(device), torch.tensor([lbl]).unsqueeze(0).to(device))\n",
    "        d1 = torch.cat((data.squeeze().cpu(), s.squeeze().cpu()), dim=1)\n",
    "    \n",
    "        d01 = model.sample(torch.tensor([[1]], device=device), (p * latent0 + (1-p) * latent1)).cpu()\n",
    "        d10 = model.sample(torch.tensor([[1]], device=device), ((1-p) * latent0 + p * latent1)).cpu()\n",
    "        interp = torch.cat((d01, d10), dim=-1).squeeze()\n",
    "    \n",
    "        print(d0.shape, d1.shape, interp.shape)\n",
    "        all = torch.cat((d0, d1, interp), dim=0)\n",
    "    \n",
    "        plt.imshow(all.cpu().squeeze().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

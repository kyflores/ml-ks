{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea98109-5559-4d4f-b1b0-fe7a090b98ee",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "The goal of this notebook is to create an educational implementation of the transformer with minimal dependencies that's easy to map to the paper.\n",
    "\n",
    "### References\n",
    "https://github.com/pytorch/examples/blob/main/word_language_model/\n",
    "\n",
    "https://github.com/karpathy/nanoGPT/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf62f3-68f0-4c07-ad7d-640638897d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Trying to avoid too many tricks and optimizations, but this makes a big difference on\n",
    "# newer NVIDIA chips without breaking anything else. It allows torch to transparently\n",
    "# use tf32 datatype in certain places.\n",
    "if device == 'cuda':\n",
    "    torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4fc8a-31d1-490a-ad50-be68a6433734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, positions, vocab_size, dmodel, embs='sin'):\n",
    "        super().__init__()\n",
    "        self.word_embs = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=dmodel\n",
    "        )\n",
    "\n",
    "        self.embs = embs\n",
    "        # This branch selects sinusoid embeddings like in the original paper\n",
    "        if self.embs == 'sin':\n",
    "            p_grid, i_grid = torch.meshgrid(\n",
    "                torch.arange(positions),\n",
    "                torch.arange(dmodel),\n",
    "                indexing='ij'\n",
    "            )\n",
    "            self.register_buffer('pos_sin', torch.sin(p_grid / (10000**(2 * i_grid / dmodel))))\n",
    "            self.register_buffer('pos_cos', torch.cos(p_grid / (10000**(2 * i_grid / dmodel))))\n",
    "        # This selects learnable embeddings.\n",
    "        else:\n",
    "            self.learnable_embs = nn.Parameter(torch.randn(positions, dmodel))\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.word_embs(x)\n",
    "        if self.embs == 'sin':\n",
    "            e = (e + self.pos_sin + self.pos_cos)\n",
    "        else:\n",
    "            e = e + self.learnable_embs\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864bf314-06d2-4c41-9f42-645b9c4d810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs q,k,v are positions x dk, or positions x dv\n",
    "# Dot all q with k. One must be tranposed to make this line up\n",
    "class SDPA(nn.Module):\n",
    "    def __init__(self, causal, positions, dkv):\n",
    "        super().__init__()\n",
    "        self.pos = positions\n",
    "        self.d = dkv\n",
    "        self.causal = causal\n",
    "        if causal:\n",
    "            self.register_buffer('mask', torch.tril(torch.ones(self.pos, self.pos)))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, q, k, v):\n",
    "        assert(self.d == k.shape[-1])\n",
    "        q_dot_k = torch.bmm(q, (k.transpose(-2, -1)))\n",
    "        scaled = q_dot_k / (math.sqrt(self.d))\n",
    "        if self.causal:\n",
    "            scaled = scaled * self.mask\n",
    "        logits = self.softmax(scaled)\n",
    "        out = torch.bmm(logits, v)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54606881-b708-4322-a171-74e963c4113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, causal, positions, dkv, dmodel):\n",
    "        super().__init__()\n",
    "        self.v_projection = nn.Linear(dmodel, dkv)\n",
    "        self.k_projection = nn.Linear(dmodel, dkv)\n",
    "        self.q_projection = nn.Linear(dmodel, dkv)\n",
    "\n",
    "        self.sdpa = SDPA(causal, positions, dkv)\n",
    "\n",
    "    def forward(self, v, k, q):\n",
    "        vp = self.v_projection(v)\n",
    "        kp = self.k_projection(k)\n",
    "        qp = self.q_projection(q)\n",
    "        return self.sdpa(qp, kp, vp)\n",
    "\n",
    "# dk = dv = dmodel/nhead\n",
    "# wq is dmodel x dk\n",
    "# wk is dmodel x dk\n",
    "# wv is dmodel x dv\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, causal, positions, nhead, dmodel):\n",
    "        super().__init__()\n",
    "        assert(dmodel % nhead == 0)\n",
    "        self.heads = nn.ModuleList([Head(causal, positions, dmodel//nhead, dmodel) for _ in range(nhead)])\n",
    "        \n",
    "\n",
    "        # TODO In the paper it's called WO, and hd_v x dmodel, but b/c d_v = dmodel/h it turns out to be square.\n",
    "        self.out_projection = nn.Linear(dmodel, dmodel)\n",
    "\n",
    "    def forward(self, v, k, q):\n",
    "        head_outputs = [h(v, k, q) for h in self.heads]\n",
    "        catd = torch.cat(head_outputs, dim=-1)\n",
    "        \n",
    "        projection = self.out_projection(catd)\n",
    "        return projection\n",
    "\n",
    "    def get_hook(self,name):\n",
    "        def hook(model, input, output):\n",
    "            self.attentions[f\"head_{name}\"] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    def setup_attention_hook(self):\n",
    "        self.attentions = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "        for i, head in enumerate(self.heads):          \n",
    "            self.hooks.append(head.sdpa.softmax.register_forward_hook(self.get_hook(i)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292bdcd-aec6-4586-947f-87ffa39dbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dmodel, ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.f0 = nn.Linear(dmodel, ff)\n",
    "        self.f1 = nn.Linear(ff, dmodel)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f0(x)\n",
    "        x = self.act(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# Input/Output are Batch, Positions, then Model Dimension.\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, positions, dmodel, ff, nhead, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha0 = MHA(True, positions, nhead, dmodel)\n",
    "        self.layernorm0 = nn.LayerNorm(dmodel)\n",
    "\n",
    "        self.feedforward = FeedForward(dmodel, ff, dropout)\n",
    "        self.layernorm2 = nn.LayerNorm(dmodel)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, inputs): \n",
    "        x = self.mha0(inputs, inputs, inputs)\n",
    "        x = self.layernorm0(x + inputs)\n",
    "\n",
    "        x_pre_ff = x\n",
    "        x = self.feedforward(x)\n",
    "        x = self.layernorm2(x + x_pre_ff)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00183a5-6513-4e7d-9062-c49a585ee372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputProjection(nn.Module):\n",
    "    def __init__(self, dmodel, vocab_size):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dmodel, vocab_size)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # You'd expect a softmax here, but we're leaving it out and going to let CrossEntropyLoss handle it instead.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad6818-a576-41a8-b47e-a22e6cf715cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, positions, dmodel, ff, nhead, nlayers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = Embeddings(positions, vocab_size, dmodel, embs='sin')\n",
    "        self.transformer_layers = nn.ModuleList([TransformerDecoder(positions, dmodel, ff, nhead=nhead, dropout=dropout) for _ in range(nlayers)])\n",
    "        self.out = OutputProjection(dmodel, vocab_size)\n",
    "\n",
    "    def setup_attention_hook(self):\n",
    "        for x in self.transformer_layers:\n",
    "            x.mha0.setup_attention_hook()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b860c9-97d5-4a98-8e6a-1674e3712161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class TextfileDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Characterlevel dataset for the a generic textfile\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, textfile='lorem_ipsum.txt', block_size=32):\n",
    "        self.length = block_size\n",
    "\n",
    "        with open(textfile, 'r') as f:\n",
    "            self.text = f.read()\n",
    "\n",
    "        self.unique_chars = set(self.text)\n",
    "        self.vocab_size = len(self.unique_chars)\n",
    "\n",
    "        # Characters to numbers\n",
    "        self.encoding = {y: x for (x, y) in enumerate(self.unique_chars)}\n",
    "        # Numbers to characters\n",
    "        self.decoding = {x: y for (x, y) in enumerate(self.unique_chars)}\n",
    "\n",
    "        self.text_encoded = torch.tensor([self.encoding[x] for x in self.text], dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 10000  # Arbitrary, samples are random slices generated at every step.\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Randomly select the part of the sequence we're going to use\n",
    "        z = torch.randint(0, len(self.text) - self.length , (1,))\n",
    "        start = z.item()\n",
    "        \n",
    "        inputs = self.text_encoded[start:start + self.length]\n",
    "        targets = self.text_encoded[start + 1 :start + 1 + self.length]\n",
    "        assert(len(targets) == self.length)\n",
    "        return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc265f-02c4-46fd-9f3c-60f666c550db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset used in minGPT, nanoGPT.\n",
    "# You can have it learn a different dataset by supplying your own text file and pointing the TextfileDataset\n",
    "# constructor to it in the next cell.\n",
    "if not os.path.exists('shakespeare.txt'):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open('shakespeare.txt', 'w') as f:\n",
    "        f.write(requests.get(data_url).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b74fcc-e28b-4a33-a99b-2113e5a096c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda':\n",
    "    dmodel = 384\n",
    "    ff = 4 * dmodel\n",
    "    heads_per_layer = 6\n",
    "    layers = 6\n",
    "    batchsize = 64\n",
    "    length = 64\n",
    "else: # Smaller model for CPU training\n",
    "    dmodel = 128\n",
    "    ff = 4 * dmodel\n",
    "    heads_per_layer = 4\n",
    "    layers = 4\n",
    "    batchsize = 16\n",
    "    length = 64\n",
    "    \n",
    "\n",
    "# Load your own large txt file here. For instance, the works of shakespeare used in Karpathy's examples.\n",
    "dset = TextfileDataset(textfile='shakespeare.txt', block_size=length)\n",
    "print(\"Dset has {} samples\".format(len(dset)))\n",
    "\n",
    "# Again, arbitrary choice of split. Because of the way our dataloader draws both train/valid samples\n",
    "# from the same text, this has no real effect.\n",
    "dset_train, dset_valid = random_split(dset, [0.9, 0.1])\n",
    "\n",
    "train_loader = DataLoader(dset_train, batch_size=batchsize, num_workers=16, shuffle=True)\n",
    "test_loader = DataLoader(dset_valid, batch_size=batchsize, num_workers=16, shuffle=True)\n",
    "\n",
    "positions = dset.get_block_size()\n",
    "vocab_size = dset.get_vocab_size()\n",
    "\n",
    "# Print some samples from the dataset\n",
    "# for x,y in dset:\n",
    "#     print('=====================================')\n",
    "#     print(''.join([dset.decoding[z] for z in x]))\n",
    "\n",
    "print(f\"Model with context size of {positions} and vocab size of {vocab_size}\")\n",
    "model = Transformer(vocab_size, positions, dmodel, ff, heads_per_layer, layers, 0.1).to(device)\n",
    "\n",
    "# Comment if this breaks on your setup, it's for performance.\n",
    "model = torch.compile(model)\n",
    "\n",
    "epochs = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "\n",
    "lossfn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d617a-4924-42d7-9560-6f2c56466073",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "for epoch in range(epochs):\n",
    "    begin = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    for i, (text, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outs = model(text.to(device))\n",
    "\n",
    "        # We're only calculating loss on the last model index.\n",
    "        # Also `target` can be passed as just token ids because CrossEntropyLoss will expand\n",
    "        # it into a onehot vector for you. \n",
    "        loss = lossfn(outs[:, -1, :], target[:, -1].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Even though the test_loader gives us samples from the same place, doing a second eval\n",
    "    # run is still different because the dropoff layers are disabled.\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (text, target) in enumerate(test_loader):\n",
    "            outs = model(text.to(device))\n",
    "            loss = lossfn(outs[:, -1, :], target[:, -1].to(device))\n",
    "            losses.append(loss)\n",
    "    elapsed = time.time() - begin\n",
    "    epoch_loss = torch.Tensor(losses).mean().item()\n",
    "    print(\"Epoch {} took {:.2f}s, Current loss is {}\".format(epoch, elapsed, epoch_loss))\n",
    "    loss_plot.append(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232c223-5f4e-42e5-90c7-69576d5389ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a55b5-959c-4815-9edc-827e391cb582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "model.eval()\n",
    "model.setup_attention_hook()\n",
    "# Start by grabbing a sample from the validation set\n",
    "buf, _ = dset_valid[0]\n",
    "print(bcolors.OKCYAN, ''.join([dset.decoding[x] for x in buf.numpy()]), bcolors.WARNING, sep='', end='')\n",
    "with torch.no_grad():\n",
    "    for x in range(length):\n",
    "        a = buf[None].to(device)\n",
    "        out = model(a)\n",
    "\n",
    "        # The prediction is the last token in the output vec, grab that and add it to the context\n",
    "        pred = F.softmax(out[0, -1, :], dim=-1)\n",
    "        pred = torch.multinomial(pred, num_samples=1).cpu()\n",
    "        print(dset.decoding[pred.item()], end='')\n",
    "        # Then drop the oldest token and keep going until we completely replaced the input.\n",
    "        buf = torch.concatenate((buf, pred))[1:]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e42d1d-d788-42ce-a3bf-734b25e3d620",
   "metadata": {},
   "source": [
    "TODO: These cells plot attention maps, but then what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39da10-bdba-4de0-b033-b95937d8dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "maps = {}\n",
    "for layer, head in itertools.product(range(layers), range(heads_per_layer)):\n",
    "    maps[(layer, head)] = model.transformer_layers[layer].mha0.attentions[f\"head_{head}\"].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552305e-a288-4780-9788-b8c17ad35ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Layers vertically, heads horizontally.\n",
    "fig, axs = plt.subplots(1, heads_per_layer, figsize=(20,5))\n",
    "for h in range(heads_per_layer):\n",
    "    axs[h].imshow(maps[(0,h)].abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4849488-59a2-470a-a497-fb740473d0da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

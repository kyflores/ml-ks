{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c59df3a-0527-4a18-b133-a7826d006294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1503d7f9-a6cb-4998-8e09-9d901c3397aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile= 'shakespeare.txt'\n",
    "with open(textfile, 'r') as f:\n",
    "    fulltext = f.read()\n",
    "\n",
    "shorttext = 'The quick brown fox jumped over the lazy dog.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0724ab8-c4f1-4ce5-a3a3-d56e0a216618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(vocab, word):\n",
    "    toks = []\n",
    "    s = word\n",
    "\n",
    "    while len(s) > 0:\n",
    "        prefix_strs = [s[0:x] for x in range(len(s) + 1)][::-1]\n",
    "        for x in prefix_strs:\n",
    "            if x in vocab:\n",
    "                toks.append(x)\n",
    "                s = s.replace(x, '', 1)\n",
    "                break\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793a8be-2604-42c6-bc72-2b4a72bf3edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = \"Eligendi et vero blanditiis a debitis.\"\n",
    "tokenize(set(blah) | {'ve', 'ro'}, \"vero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ae1c2-717c-4849-af4d-f0f2a4e7885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(text, vocab_limit):\n",
    "    vocab = set(text)\n",
    "    words = text.split(' ')\n",
    "\n",
    "    while(len(vocab) < vocab_limit):\n",
    "        # First, tokenize the data with the existing vocab, then generate all pairs\n",
    "        # as proposed new tokens\n",
    "        new_toks = set()\n",
    "        for w in words:\n",
    "            toks = tokenize(vocab, w)\n",
    "            pairs = [''.join(x) for x in zip(toks[:-1], toks[1:])]\n",
    "            new_toks.update(pairs)\n",
    "    \n",
    "        # Next, retokenize with the new pairs added to the vocab, counting the number of occurrences of each new tok.\n",
    "        stats = {x:0 for x in new_toks}\n",
    "        for w in words:\n",
    "            toks = tokenize(vocab | new_toks, w)\n",
    "            for t in toks:\n",
    "                if t in stats.keys():\n",
    "                    stats[t] += 1\n",
    "\n",
    "        # Look at how often each proposed token was used, and add the most frequent one to the vocab.\n",
    "        tmp = [(stats[x], x) for x in stats]\n",
    "        tmp.sort(key=lambda x: x[0])\n",
    "\n",
    "        # Traverse the sorted list backwards to find the tokens with the most occurrences.\n",
    "        # Go 4 at a time to speed things along\n",
    "        for x in range(1, 5, 1):\n",
    "            k, v = tmp[-x]\n",
    "            vocab.update([v])\n",
    "            print(\"New token >> {} << which occurred {} times\".format(v, k))\n",
    "        \n",
    "        print(\"Vocab size \", len(vocab))\n",
    "\n",
    "    # Stop when vocab reaches the target size.\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481e566-d6fc-4d45-bf32-d2442a1d9b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_vocab = train(fulltext, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaddfa2-9c18-44b5-94f0-fb650096e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(trained_vocab, \"We are accounted poor citizens, the patricians good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7453c5f-7879-4b91-a507-1dfd02ac3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_str = {i: x for i, x in enumerate(list(trained_vocab))}\n",
    "str_to_id = {v: k for k, v in id_to_str.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d47875-330c-4ac0-9a8c-d2dba5ab3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('tokenizer.pkl', 'wb') as t:\n",
    "    pkl.dump({'id2str': id_to_str, 'str2id': str_to_id}, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

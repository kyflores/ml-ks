{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c2ba790-12aa-4055-8f6f-a55020d0d3b2",
   "metadata": {},
   "source": [
    "# A miniature implementation of ViT\n",
    "Train your own hackable ViT model with vanilla pytorch\n",
    "\n",
    "## Details\n",
    "* 843k parameters as written, needing 1.3GB VRAM. ~2:30 per epoch on 8-core CPU, ~0:10 on 3050ti.\n",
    "* Expected to reach ~70%+ classification accuracy on CIFAR10.\n",
    "* Model dimensions have been adjusted to train quickly and work with 32x32 pixel images.\n",
    "* Builds the transformer from basic layers and does not use torch's transformer blocks.\n",
    "* Comparable final accuracy to an implementation using torch's transformer.\n",
    "\n",
    "## References\n",
    "https://github.com/tintn/vision-transformer-from-scratch\n",
    "\n",
    "https://arxiv.org/abs/2010.11929 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687354e-98f1-4761-9c6a-a3bd6e06f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = './datasets'\n",
    "print('Will store datasets in', dataset_root)\n",
    "\n",
    "import os\n",
    "cpu_num = os.cpu_count() // 2\n",
    "print('Dataloaders will use {} CPUs'.format(cpu_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10faec3f-1468-40a6-bc17-61e743e7815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms.v2 as tv2\n",
    "import torchvision.datasets as tds\n",
    "import torchvision.utils as tu\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e2991a-6683-4273-bd66-ea8e9fd7282d",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487af93-7bec-43fe-b273-8216a5e19fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only apply the most basic augmentations.\n",
    "train_tfs = tv2.Compose([\n",
    "    tv2.ToImage(),\n",
    "    tv2.RandomCrop(32, 4),\n",
    "    tv2.RandomHorizontalFlip(0.5),\n",
    "    tv2.RandomVerticalFlip(0.25),\n",
    "    tv2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "# Don't apply augmentations on the validation set.\n",
    "val_tfs = tv2.Compose([\n",
    "    tv2.ToImage(),\n",
    "    tv2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "cifar_train = tds.CIFAR10(\n",
    "    root=dataset_root, download=True, train=True, transform=train_tfs)\n",
    "cifar_eval = tds.CIFAR10(\n",
    "    root=dataset_root, download=True, train=False, transform=val_tfs)\n",
    "cifar_train = tds.wrap_dataset_for_transforms_v2(cifar_train)\n",
    "cifar_eval = tds.wrap_dataset_for_transforms_v2(cifar_eval)\n",
    "\n",
    "batchsize = 256\n",
    "train_loader = tud.DataLoader(cifar_train, batch_size=batchsize, num_workers=cpu_num, shuffle=True)\n",
    "val_loader = tud.DataLoader(cifar_eval, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e519629c-a812-47d0-b701-22d51d5f6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_grid(imgs, sz: int):\n",
    "    grid = tu.make_grid(imgs)\n",
    "    return grid.permute(1, 2, 0)\n",
    "\n",
    "num=64\n",
    "augmented = torch.stack([x[0] for x in random.choices(cifar_train, k=num)])\n",
    "print(augmented.shape, augmented.mean())\n",
    "tmps = random_grid(augmented, num)\n",
    "plt.imshow(tmps.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56555aa-7551-4dfd-8a04-e155cc969820",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7192c-caaf-4158-ac54-6590f4a8be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDPA(nn.Module):\n",
    "    def __init__(self, positions, dkv):\n",
    "        super().__init__()\n",
    "        self.pos = positions\n",
    "        self.d = dkv\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        assert(self.d == k.shape[-1])\n",
    "        q_dot_k = torch.bmm(q, (k.transpose(-2, -1)))\n",
    "        scaled = q_dot_k / (math.sqrt(self.d))\n",
    "        logits = self.softmax(scaled)\n",
    "        out = torch.bmm(logits, v)\n",
    "        return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, positions, dkv, dmodel):\n",
    "        super().__init__()\n",
    "        self.v_projection = nn.Linear(dmodel, dkv)\n",
    "        self.k_projection = nn.Linear(dmodel, dkv)\n",
    "        self.q_projection = nn.Linear(dmodel, dkv)\n",
    "\n",
    "        self.sdpa = SDPA(positions, dkv)\n",
    "\n",
    "    def forward(self, v, k, q):\n",
    "        vp = self.v_projection(v)\n",
    "        kp = self.k_projection(k)\n",
    "        qp = self.q_projection(q)\n",
    "        return self.sdpa(qp, kp, vp)\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, positions, nhead, dmodel):\n",
    "        super().__init__()\n",
    "        assert(dmodel % nhead == 0)\n",
    "        self.heads = nn.ModuleList([Head(positions, dmodel//nhead, dmodel) for _ in range(nhead)])\n",
    "\n",
    "        # TODO In the paper it's called WO, and hd_v x dmodel, but b/c d_v = dmodel/h it turns out to be square.\n",
    "        self.out_projection = nn.Linear(dmodel, dmodel)\n",
    "\n",
    "    def forward(self, v, k, q):\n",
    "        head_outputs = [h(v, k, q) for h in self.heads]\n",
    "        catd = torch.cat(head_outputs, dim=-1)\n",
    "\n",
    "        projection = self.out_projection(catd)\n",
    "        return projection\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dmodel, ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.f0 = nn.Linear(dmodel, ff)\n",
    "        self.f1 = nn.Linear(ff, dmodel)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f0(x)\n",
    "        x = self.act(x)\n",
    "        x = self.f1(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# Input/Output are Batch, Positions, then Model Dimension.\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, positions, dmodel, ff, nhead, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MHA(positions, nhead, dmodel)\n",
    "        self.layernorm0 = nn.LayerNorm(dmodel)\n",
    "        self.feedforward = FeedForward(dmodel, ff, dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(dmodel)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.mha(inputs, inputs, inputs)\n",
    "        x = self.layernorm0(x + inputs)\n",
    "\n",
    "        x_pre_ff = x\n",
    "        x = self.feedforward(x)\n",
    "        x = self.layernorm1(x + x_pre_ff)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, positions, dmodel, ff, nhead, nlayers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.transformer_layers = nn.ModuleList([TransformerBlock(positions, dmodel, ff, nhead=nhead, dropout=dropout) for _ in range(nlayers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9270f5-73dc-47b9-ba33-af52988da127",
   "metadata": {},
   "source": [
    "# ViT Specific layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031213c6-9dcc-4a27-bf44-f419c29e7d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input B C H W\n",
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self, c_in, emb_len, patch_dim):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(c_in, emb_len, patch_dim, stride=patch_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "# Input B, (P * P), C\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, emb_len, patch_num, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # nn.Parameter is used so that PyTorch knows this is part of the network.\n",
    "        # Specifically it makes this value appear whenever model.parameters() is called.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_len))\n",
    "        self.pos_emb = nn.Parameter(\n",
    "            torch.randn(1, patch_num + 1, emb_len)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, p2, c = x.shape\n",
    "\n",
    "        cls_tok = self.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tok, x), dim=1)\n",
    "        x = x + self.pos_emb\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class ClassHead(nn.Module):\n",
    "    def __init__(self, emb_len, patch_num, hidden_sz, classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc0 = nn.Linear(emb_len, hidden_sz)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(hidden_sz, classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # We're slicing out the CLS token slot here. This is the only slot that matters.\n",
    "        x = inp[:, 0, :]\n",
    "        x = self.fc0(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class VIT(nn.Module):\n",
    "    def __init__(self, num_classes, patchsz, emb_dim, layers=4, heads=4, dim_ff=64, load=None):\n",
    "        super().__init__()\n",
    "        self.tokenizer = Tokenizer(3, emb_dim, patchsz)\n",
    "        self.embeddings = Embeddings(emb_dim, patch_num, 0.1) # 64 = (32 / 4)**2\n",
    "        self.transformer = Transformer(patch_num + 1, emb_dim, dim_ff, heads, layers, dropout=0.1)\n",
    "        self.class_head = ClassHead(emb_dim, patch_num + 1, 256, num_classes)\n",
    "        if load and os.path.isfile(load):\n",
    "            print(f\"Will load weights from {load}\")\n",
    "            self.load_state_dict(torch.load(load))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        x = self.embeddings(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.class_head(x)\n",
    "        return x\n",
    "\n",
    "    def save(self, name='vit_classification.pth'):\n",
    "        torch.save(self.state_dict(), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478a4bb-90fe-4010-a6ef-b4b45d505609",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d610b-31f4-4bb3-a412-9015ab1ac43a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "imgsz = 32  # Images are this many pixels tall and wide in CIFAR10\n",
    "patchsz = 4 # Patchsize of 4 results in 8x8 patches,\n",
    "emb_dim = 128  # AKA dmodel. Try reduce this if it takes too long to train.\n",
    "patch_num = (imgsz // patchsz)**2\n",
    "vit = VIT(\n",
    "    10,\n",
    "    patchsz,\n",
    "    emb_dim,\n",
    "    layers=4,\n",
    "    heads=4,\n",
    "    dim_ff=4*128,\n",
    "    load='vit_classification.pth'  # Set to the saved model path to start from a checkpoint.\n",
    ").to(device)\n",
    "\n",
    "# Comment this out and rerun the cell if it errors, but if it works it improves performance.\n",
    "# If compiling expect the first run to be slow.\n",
    "vit = torch.compile(vit)\n",
    "\n",
    "# !pip install torchinfo\n",
    "try:\n",
    "    from torchinfo import summary; print(summary(vit, (1, 3, 32, 32), device=device))\n",
    "except ImportError:\n",
    "    print(\"torchinfo isn't installed. Try `pip install torchinfo` to see a summary of the model shapes and param counts\")\n",
    "\n",
    "epochs = 100\n",
    "optimizer = torch.optim.AdamW(vit.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_plot = []\n",
    "for epoch in range(epochs):\n",
    "    vit.train()\n",
    "    for i, (images, target) in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        images = images.float().to(device)\n",
    "        targets = target.to(device)\n",
    "\n",
    "        outs = vit(images)\n",
    "\n",
    "        # If you're wondering why there's no (Log)SoftMax here or in the network, it's because\n",
    "        # PyTorch does it for you inside the CrossEntropyLoss operation.\n",
    "        loss = lossfn(outs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses = []\n",
    "    vit.eval() # Certain layers like dropout behave differently during evaluation vs training.\n",
    "    correct = 0\n",
    "    total = len(cifar_eval)\n",
    "    for i, (images, target) in enumerate(tqdm(val_loader)):\n",
    "        with torch.no_grad():\n",
    "            images = images.float().to(device)\n",
    "            targets = target.to(device)\n",
    "            outs = vit(images)\n",
    "            loss = lossfn(outs, targets)\n",
    "            losses.append(loss)\n",
    "            for x in range(outs.shape[0]):\n",
    "                preds = F.softmax(outs, dim=1)\n",
    "                cls = preds[x].argmax()\n",
    "                lbl = targets[x]\n",
    "                if cls == lbl:\n",
    "                    correct += 1\n",
    "\n",
    "    epoch_loss = torch.Tensor(losses).mean().item()\n",
    "    print(\"Epoch {}, Current loss is {}\".format(epoch, epoch_loss))\n",
    "    print(\"{}/{} correct, {:.2f}%\".format(correct, total, 100*correct/total))\n",
    "    loss_plot.append(epoch_loss)\n",
    "\n",
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a371235-3813-4008-912a-78f49bb5826a",
   "metadata": {},
   "source": [
    "# Evaluation with Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e9f54-c0e7-4fc9-a505-c16074ecbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "from sklearn import metrics\n",
    "\n",
    "evals = cifar_eval\n",
    "right = 0; total = 0; y_pred=[]; y_true=[]\n",
    "\n",
    "vit.eval()\n",
    "with torch.no_grad():\n",
    "    for image, target in tqdm(cifar_eval):\n",
    "        pred = vit(image.unsqueeze(0).float().to(device))\n",
    "        pred = F.softmax(pred, dim=1).argmax()\n",
    "\n",
    "        y_pred.append(pred.item())\n",
    "        y_true.append(target)\n",
    "\n",
    "metrics.ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5478b9c-1d62-4cc3-91b8-37cffdc7f458",
   "metadata": {},
   "source": [
    "Here are your CIFAR10 classes for reference\n",
    "```\n",
    "0:\"Airplane\"\n",
    "1:\"Automobile\"\n",
    "2:\"Bird\"\n",
    "3:\"Cat\"\n",
    "4:\"Deer\"\n",
    "5:\"Dog\"\n",
    "6:\"Frog\"\n",
    "7:\"Horse\"\n",
    "8:\"Ship\"\n",
    "9:\"Truck\"\n",
    "```\n",
    "Your confusion matrix will likely show error between 3 and 5, cats and dogs, which seems reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934128bc-9e44-4212-9472-26d1c2419013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save your weights.\n",
    "# vit.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fa901-46b7-4afb-90c8-a8cb11c24a1f",
   "metadata": {},
   "source": [
    "# Some ideas for improving classification performance\n",
    "* Increase model dimensions. More attention heads, more sequential transformers, bigger feedforward, different patch sizes.\n",
    "* Add augmentations to the dataset transform functions. `RandomResizedCrop` would be a good choice.\n",
    "* Normalize the images to mean=0 std=1. Right now they're just scaled to be between 0 and 1.\n",
    "* Introduce a learning rate schedule to cool the learning rate as training progresses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602de28c-f6c8-4c92-8845-7d4766b156a5",
   "metadata": {},
   "source": [
    "# Train LoRAs with HuggingFace APIs\n",
    "\n",
    "### Install\n",
    "pip install pytorch transformers datasets peft jupyterlab ipywidgets\n",
    "\n",
    "### Notes\n",
    "grimm = dts.load_dataset(\"Eugenememe/grimms\")\n",
    "\n",
    "Merging LoRA can be achieved with [add_weighted_adapter](https://huggingface.co/docs/peft/package_reference/lora#peft.LoraModel.add_weighted_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c3d8c-b1db-4ece-a228-1d0342d5ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this var to download everything to the directory where this notebook is.\n",
    "# Goes under \"./hub\"\n",
    "%env HF_HOME=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf45e1-b250-4606-8df5-2532646faa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as tfs\n",
    "import datasets as dts\n",
    "import accelerate\n",
    "import peft\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714817c-2fb3-4176-96a0-d85bdaa704fc",
   "metadata": {},
   "source": [
    "This cell simply fetches the model from Hugging Face Hub. We're using their SmolLM-135M model here, which has 135M parameters and a context window of 2048. However, we're limited the size of all our data to 1024 to limit memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1157d9f8-06da-44e7-befd-0c9260952ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "smol_lm = \"HuggingFaceTB/SmolLM-135M\"\n",
    "# smol_lm = \"HuggingFaceTB/SmolLM-360M\"\n",
    "# smol_lm = \"HuggingFaceTB/SmolLM-1.7B\"\n",
    "amd_slm = \"amd/AMD-Llama-135m\"\n",
    "\n",
    "def load_model(name: str):\n",
    "    MAX_LEN=1024 # Reduce the usable context size to save VRAM\n",
    "    \n",
    "    config = tfs.AutoConfig.from_pretrained(name)\n",
    "    model = tfs.AutoModelForCausalLM.from_pretrained(name)\n",
    "    tokenizer = tfs.AutoTokenizer.from_pretrained(\n",
    "        name,\n",
    "        model_max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "    # TODO Required for PEFT to use gradient checkpointing https://github.com/huggingface/peft/issues/137\n",
    "    # model.enable_input_require_grads()\n",
    "        \n",
    "    # chatml - requires <|im_start|> and <|im_end|> special tokens.\n",
    "    # If they don't exist, tokenizer.add_special_tokens and model.resize_token_embeddings can be used, but\n",
    "    # these tokens would come with randomly initialized embeddings and need finetuning.\n",
    "    # Standard LoRA does not train input embeddings so this probably won't work without full fine tune.\n",
    "    # See for details on chat template https://huggingface.co/docs/transformers/main/chat_templating#what-template-should-i-use\n",
    "    tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "    # Using eos as the pad token seems common practice.\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return config, model, tokenizer\n",
    "\n",
    "cfg, mdl, tok = load_model(smol_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831a5c5-a8c6-406f-9242-14282f16f4cb",
   "metadata": {},
   "source": [
    "# Instruct LoRA\n",
    "First we'll train a LoRA for instruction following using the dolly-15k dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ae321-d8c6-4f71-ba34-104ca1d3dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruct dataset. A possible alternative is \"tatsu-lab/alpaca\"\n",
    "dolly = dts.load_dataset(\"databricks/databricks-dolly-15k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd94d2-d9ff-4a92-944f-2c76ed9262de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok is captured from the global namespace\n",
    "# x keys are instruction, context, response, category\n",
    "def dolly_chat(x):\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"{}\".format(x[\"context\"])},\n",
    "        {\"role\": \"user\", \"content\": \"{}\".format(x[\"instruction\"])},\n",
    "        {\"role\": \"assistant\", \"content\": \"{}\".format(x[\"response\"])},\n",
    "    ]\n",
    "    chat_formatted = tok.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    # TODO Padding to max length always seems to result in static VRAM usage, but\n",
    "    # is slower on average since many samples are much shorter than max_length.\n",
    "    # Want to debug why peak VRAM fluctuates a lot when length can vary, as this sometimes\n",
    "    # OOMs midway through training.\n",
    "    tokenized = tok(chat_formatted, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    return {\"text\": chat_formatted, \"input_ids\": tokenized[\"input_ids\"]}\n",
    "\n",
    "dset_w_tokenized = dolly[\"train\"].map(dolly_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c3b52-4918-4ad7-af41-f207c9119038",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dset_w_tokenized[\"input_ids\"][0]))\n",
    "print(dset_w_tokenized[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd3a1b-3b18-4016-a915-be998c136768",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 32\n",
    "\n",
    "# I got much better results by training embed_tokens. It's possible <|im_start|> <|im_end|> never\n",
    "# appeared in the pretraining, but were included in the tokenizer anyway.\n",
    "lora_config = peft.LoraConfig(\n",
    "    r=rank, # This is the \"rank\"\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"embed_tokens\"],\n",
    "    task_type=peft.TaskType.CAUSAL_LM,\n",
    "    lora_alpha=rank, # Rule of thumb seems to be 1-2x the rank.\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "lora_model = peft.get_peft_model(mdl, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564bce2-ffbf-464e-8e52-2cf651f95007",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dset_w_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f0676-c7e5-41d2-b305-a3bd7ab65495",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-5\n",
    "batchsize=8\n",
    "epochs=1\n",
    "\n",
    "args = tfs.TrainingArguments(\n",
    "    output_dir='./finetune',\n",
    "    optim='adamw_torch',\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batchsize,\n",
    "    per_device_eval_batch_size=batchsize,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,\n",
    "    save_strategy=\"no\",\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    report_to='none',\n",
    "    torch_empty_cache_steps=100,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    ")\n",
    "\n",
    "collator = tfs.DataCollatorForLanguageModeling(tok, mlm=False)\n",
    "\n",
    "trainer = tfs.Trainer(\n",
    "    model=lora_model,\n",
    "    args=args,\n",
    "    train_dataset=dset_w_tokenized,\n",
    "    processing_class=tok,\n",
    "    data_collator=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421ee62-878c-4aa5-a32e-9f78b78b5fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c027431-9229-4577-8d41-c99607f105d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the wikipedia page for owls.\n",
    "sys = \"From beak to tail, an American crow measures 40–50 cm (16–20 in), almost half of which is tail. Its wingspan is 85–100 cm (33–39 in). Mass varies from about 300 to 600 g (11 to 21 oz), with males tending to be larger than females. Plumage is all black, with iridescent feathers. It looks much like other all-black corvids. They are very intelligent, and adaptable to human environments. The most usual call is CaaW!-CaaW!-CaaW! They can be distinguished from the common raven (C. corax) because American crows are smaller and the beak is slightly less pronounced; from the fish crow (C. ossifragus) because American crows do not hunch and fluff their throat feathers when they call; and from the carrion crow (C. corone) by size, as the carrion crow is larger and of a stockier build. \"\n",
    "question = \"Can you tell me about crows?\"\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": f\"{sys}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "]\n",
    "\n",
    "text = tok.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tok(text, return_tensors='pt', truncation=True).to(lora_model.device)\n",
    "print(\"Prompt has\", len(inputs[\"input_ids\"][0]), \"tokens\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = lora_model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "        # Change the following 4 parameters to control how the outputs are sampled.\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.5,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.10,\n",
    "    )\n",
    "    \n",
    "    output = tok.batch_decode(output)[0]\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa2b94-afb1-403e-a71c-5be3f79c12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(save_directory=\"mlhi-lora-instruct\", save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1effa5-d60e-4be6-be70-52011e9b958d",
   "metadata": {},
   "source": [
    "# Corpus LoRA\n",
    "Next we're going to try a separate LoRA on the same base model with a different dataset, a text dump of all the Grimm Fairytale stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a4d78-34bb-4ed2-821e-8c6524204bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a fresh copy of the model\n",
    "cfg, mdl, tok = load_model(smol_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb00daf-a125-4c0f-8588-0d08bedc7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grimm = dts.load_dataset(\"Eugenememe/grimms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef714f7-d99d-438f-b953-9ce89e0fcfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grimm_corpus(x):\n",
    "    story = tok(x[\"story\"], padding=\"max_length\", truncation=False)\n",
    "\n",
    "    return {\"text\": story, \"input_ids\": story[\"input_ids\"]}\n",
    "\n",
    "grimm_tokenized = grimm[\"train\"].map(grimm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dcff25-7691-4e4f-a6ed-f84e3981c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/huggingface/transformers/issues/18075\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RandomCropDataCollator(tfs.DataCollatorForLanguageModeling):\n",
    "    random_truncation_token_length: int = 1024\n",
    "\n",
    "    def __call__(self, features):\n",
    "        for f in features:\n",
    "            original_token_length = len(f['input_ids'])\n",
    "\n",
    "            if self.random_truncation_token_length < original_token_length:\n",
    "                start_truncation = random.randint(0, original_token_length-self.random_truncation_token_length)\n",
    "                # print(original_token_length, start_truncation)\n",
    "                # f['input_ids'] = f['input_ids'][:start_truncation] + f['input_ids'][start_truncation+self.random_truncation_token_length:]\n",
    "                f['input_ids'] = f['input_ids'][start_truncation : start_truncation+self.random_truncation_token_length]\n",
    "                # f['attention_mask'] = f['attention_mask'][:start_truncation] + f['attention_mask'][start_truncation+self.random_truncation_token_length:]\n",
    "                end_shape = len(f['input_ids'])\n",
    "                # print(original_token_length, \"-------->\", end_shape)\n",
    "        return super().__call__(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead25bc-68c4-4b22-9acc-4a7c3b630f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_collator = RandomCropDataCollator(\n",
    "    tok,\n",
    "    random_truncation_token_length=tok.model_max_length,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b0694d-1cf4-46cc-9aa8-c91f4144d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = peft.LoraConfig(\n",
    "    r=rank, # Keep this rank the same as the instruct model\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=peft.TaskType.CAUSAL_LM,\n",
    "    lora_alpha=rank,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "\n",
    "lora_model = peft.get_peft_model(mdl, lora_config)\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "lr=1e-5\n",
    "batchsize=8\n",
    "epochs=10\n",
    "\n",
    "args = tfs.TrainingArguments(\n",
    "    output_dir='./finetune',\n",
    "    optim='adamw_torch',\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batchsize,\n",
    "    per_device_eval_batch_size=batchsize,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,\n",
    "    save_strategy=\"no\",\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    report_to='none',\n",
    "    torch_empty_cache_steps=100,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    ")\n",
    "\n",
    "trainer = tfs.Trainer(\n",
    "    model=lora_model,\n",
    "    args=args,\n",
    "    train_dataset=grimm_tokenized,\n",
    "    processing_class=tok,\n",
    "    data_collator=crop_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad70c1-3fa5-4b71-b42f-f30513a2ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391605d3-b279-4504-9d72-6d61c0d7248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Once upon a time,\"\n",
    "inputs = tok(text, return_tensors='pt', truncation=True).to(lora_model.device)\n",
    "print(\"Prompt has\", len(inputs[\"input_ids\"][0]), \"tokens\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = lora_model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "        # Change the following 4 parameters to control how the outputs are sampled.\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.5,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.10,\n",
    "    )\n",
    "    \n",
    "    output = tok.batch_decode(output)[0]\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37676ca1-2bf2-4348-9bf0-82ec9d900007",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(save_directory=\"mlhi-lora-corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed3ba2-f87e-4374-8fb4-737a874b1b22",
   "metadata": {},
   "source": [
    "# Loading LoRA back for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1216acf-bf73-4c54-9625-ce1245d1b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a fresh copy of the model\n",
    "cfg, mdl, tok = load_model(smol_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9bd461-2f9a-4f1c-85de-3b21a6562db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapted_model = peft.PeftModel.from_pretrained(mdl, \"mlhi-lora-instruct\", adapter_name=\"mlhi-lora-instruct\")\n",
    "print(adapted_model.active_adapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b17e2-7e0d-4a8a-9c66-c971f72ab36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapted_model.load_adapter(\"mlhi-lora-corpus\", adapter_name=\"mlhi-lora-corpus\")\n",
    "adapters = [\"mlhi-lora-corpus\", \"mlhi-lora-instruct\"]\n",
    "weights = [0.5, 0.5]\n",
    "adapter_name = \"merged\"\n",
    "adapted_model.add_weighted_adapter(adapters, weights, adapter_name, combination_type=\"svd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79daa65f-d15f-4c01-b198-ac8f25a802c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapted_model.set_adapter(\"merged\")\n",
    "print(adapted_model.active_adapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be93158-3fca-449b-ad2f-1cdd95e5b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys=\"\"\n",
    "question = \"Can you summarize the story of Cinderella?\"\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": f\"{sys}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "]\n",
    "\n",
    "text = tok.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tok(text, return_tensors='pt', truncation=True).to(adapted_model.device)\n",
    "print(\"Prompt has\", len(inputs[\"input_ids\"][0]), \"tokens\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = mdl.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "        # Change the following 4 parameters to control how the outputs are sampled.\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.5,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.10,\n",
    "    )\n",
    "    \n",
    "    output = tok.batch_decode(output)[0]\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c62b4-538b-416c-bcfe-7072a7e197e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

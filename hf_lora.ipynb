{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602de28c-f6c8-4c92-8845-7d4766b156a5",
   "metadata": {},
   "source": [
    "# Train LoRAs with HuggingFace APIs\n",
    "\n",
    "### Install\n",
    "pip install pytorch transformers datasets peft jupyterlab ipywidgets\n",
    "\n",
    "### Notes\n",
    "grimm = dts.load_dataset(\"Eugenememe/grimms\")\n",
    "\n",
    "Merging LoRA can be achieved with [add_weighted_adapter](https://huggingface.co/docs/peft/package_reference/lora#peft.LoraModel.add_weighted_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8c3d8c-b1db-4ece-a228-1d0342d5ee17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HOME=.\n"
     ]
    }
   ],
   "source": [
    "# Set this var to download everything to the directory where this notebook is.\n",
    "# Goes under \"./hub\"\n",
    "%env HF_HOME=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7faf45e1-b250-4606-8df5-2532646faa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as tfs\n",
    "import datasets as dts\n",
    "import accelerate\n",
    "import peft\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714817c-2fb3-4176-96a0-d85bdaa704fc",
   "metadata": {},
   "source": [
    "I couldn't find many options under 1B, but we want something small so we can train on limited hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1157d9f8-06da-44e7-befd-0c9260952ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "amd_slm = \"amd/AMD-Llama-135m\"\n",
    "\n",
    "def load_model(name: str):\n",
    "    config = tfs.AutoConfig.from_pretrained(name)\n",
    "    model = tfs.AutoModelForCausalLM.from_pretrained(name)\n",
    "    tokenizer = tfs.AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "    # Required for PEFT to use gradient checkpointing https://github.com/huggingface/peft/issues/137\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    # This method is probably not suitable for basic LoRA b/c it requires adding new tokens.\n",
    "    if False:\n",
    "        # One of our LoRAs will teach the model a chat/instruct format, so we need to add the marker tokens used by chat_ml.\n",
    "        # These tokens will be randomly initialized and trained by the LoRA.\n",
    "        tokenizer.add_special_tokens(special_tokens_dict={\"pad_token\":\"<|pad|>\",\"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]})\n",
    "    \n",
    "        # Chatml\n",
    "        tokenizer.chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}\"\n",
    "    \n",
    "        # Extend the model's token embedding matrix to add the new tokens.\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "    # Mistral\n",
    "    tokenizer.chat_template = \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ ' [INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"\n",
    "    # TODO Not sure if this is okay, but LoRA does not train input embeddings, so I don't want to introduce a new token.\n",
    "    # Maybe we can just find another model with a pad token.\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return config, model, tokenizer\n",
    "\n",
    "# I chose AMD-Llama-135m because...\n",
    "# - Small, there aren't many models in the 100m range.\n",
    "# - 2048 context window\n",
    "# - Llama-like\n",
    "# Most other models on HF hub should work with this notebook.\n",
    "cfg, mdl, tok = load_model(amd_slm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e34d0a0-1fa3-472c-b7d6-b9af161368e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruct dataset. A possible alternative is \"tatsu-lab/alpaca\"\n",
    "dolly = dts.load_dataset(\"databricks/databricks-dolly-15k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffcd94d2-d9ff-4a92-944f-2c76ed9262de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423373cb491f4110bbff16b068a3f602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# tok captured from the global namespace\n",
    "# x keys are instruction, context, response, category\n",
    "def dolly_chat(x):\n",
    "    instruction = tok(x[\"instruction\"], truncation=True)\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": \"{}\".format(x[\"instruction\"])},\n",
    "        {\"role\": \"assistant\", \"content\": \"{}\".format(x[\"response\"])},\n",
    "    ]\n",
    "    chat_formatted = tok.apply_chat_template(chat, tokenize=False, add_generate_prompt=True)\n",
    "    tokenized = tok.apply_chat_template(chat, tokenize=True, add_generate_prompt=True)\n",
    "\n",
    "    return {\"text\": chat_formatted, \"input_ids\": tokenized}\n",
    "\n",
    "dset_w_tokenized = dolly[\"train\"].map(dolly_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70cd3a1b-3b18-4016-a915-be998c136768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 134,400,768 || trainable%: 0.2194\n"
     ]
    }
   ],
   "source": [
    "lora_config = peft.LoraConfig(\n",
    "    r=8, # This is the \"rank\"\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=peft.TaskType.CAUSAL_LM,\n",
    "    lora_alpha=16, # Rule of thumb seems to be 2x the rank.\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "lora_model = peft.get_peft_model(mdl, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e564bce2-ffbf-464e-8e52-2cf651f95007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15011"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dset_w_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c7f0676-c7e5-41d2-b305-a3bd7ab65495",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-4\n",
    "batchsize=4\n",
    "epochs=3\n",
    "\n",
    "args = tfs.TrainingArguments(\n",
    "    output_dir='./finetune',\n",
    "    optim='adamw_torch',\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batchsize,\n",
    "    per_device_eval_batch_size=batchsize,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    save_strategy=\"no\",\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    report_to='none',\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    ")\n",
    "\n",
    "collator = tfs.DataCollatorForLanguageModeling(tok, mlm=False)\n",
    "\n",
    "trainer = tfs.Trainer(\n",
    "    model=lora_model,\n",
    "    args=args,\n",
    "    train_dataset=dset_w_tokenized,\n",
    "    processing_class=tok,\n",
    "    data_collator=collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421ee62-878c-4aa5-a32e-9f78b78b5fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='2814' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  17/2814 00:01 < 05:14, 8.89 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c027431-9229-4577-8d41-c99607f105d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys = \"Owls are birds from the order Strigiformes which includes over 200 species of mostly solitary and nocturnal birds of prey typified by an upright stance, a large, broad head, binocular vision, binaural hearing, sharp talons, and feathers adapted for silent flight. Exceptions include the diurnal northern hawk-owl and the gregarious burrowing owl. \"\n",
    "question = \"What are some common qualities of owls?\"\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "]\n",
    "\n",
    "text = tok.apply_chat_template(chat, tokenize=False, add_generate_prompt=True)\n",
    "inputs = tok(text, return_tensors='pt', truncation=True).to(lora_model.device)\n",
    "print(type(inputs))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = lora_model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tok.pad_token_id,\n",
    "        # Change the following 4 parameters to control how the outputs are sampled.\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.50,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.10,\n",
    "    )\n",
    "    \n",
    "    output = tok.batch_decode(output)[0]\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa2b94-afb1-403e-a71c-5be3f79c12ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

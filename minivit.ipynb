{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c2ba790-12aa-4055-8f6f-a55020d0d3b2",
   "metadata": {},
   "source": [
    "# A miniature implementation of ViT\n",
    "\n",
    "## References\n",
    "https://github.com/tintn/vision-transformer-from-scratch\n",
    "\n",
    "https://arxiv.org/abs/2010.11929 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687354e-98f1-4761-9c6a-a3bd6e06f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = './datasets'\n",
    "print('Will store datasets in', dataset_root)\n",
    "\n",
    "import os\n",
    "cpu_num = os.cpu_count() // 2\n",
    "print('Dataloaders will use {} CPUs'.format(cpu_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10faec3f-1468-40a6-bc17-61e743e7815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms.v2 as tv2\n",
    "import torchvision.datasets as tds\n",
    "import torchvision.utils as tu\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487af93-7bec-43fe-b273-8216a5e19fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfs = tv2.Compose([\n",
    "    tv2.ToImage(),\n",
    "    tv2.RandomCrop(32, 4),\n",
    "    tv2.RandomHorizontalFlip(0.5),\n",
    "    tv2.RandomVerticalFlip(0.25),\n",
    "    tv2.ToDtype(torch.float32, scale=True),\n",
    "    tv2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_tfs = tv2.Compose([\n",
    "    tv2.ToImage(),\n",
    "    tv2.ToDtype(torch.float32, scale=True),\n",
    "    tv2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "cifar_train = tds.CIFAR10(\n",
    "    root=dataset_root, download=True, train=True, transform=train_tfs)\n",
    "cifar_eval = tds.CIFAR10(\n",
    "    root=dataset_root, download=True, train=False, transform=val_tfs)\n",
    "cifar_train = tds.wrap_dataset_for_transforms_v2(cifar_train)\n",
    "cifar_eval = tds.wrap_dataset_for_transforms_v2(cifar_eval)\n",
    "\n",
    "batchsize = 256\n",
    "train_loader = tud.DataLoader(cifar_train, batch_size=batchsize, num_workers=cpu_num, shuffle=True)\n",
    "val_loader = tud.DataLoader(cifar_eval, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e519629c-a812-47d0-b701-22d51d5f6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_grid(imgs, sz: int):\n",
    "    grid = tu.make_grid(imgs)\n",
    "    return grid.permute(1, 2, 0)\n",
    "\n",
    "num=64\n",
    "augmented = torch.stack([x[0] for x in random.choices(cifar_train, k=num)])\n",
    "print(augmented.shape, augmented.mean())\n",
    "tmps = random_grid(augmented, num)\n",
    "plt.imshow(tmps.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031213c6-9dcc-4a27-bf44-f419c29e7d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input B C H W\n",
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self, c_in, emb_len, patch_dim):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(c_in, emb_len, patch_dim, stride=patch_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "# Input B, (P * P), C\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, emb_len, patch_num, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_len))\n",
    "        self.pos_emb = nn.Parameter(\n",
    "            torch.randn(1, patch_num + 1, emb_len)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, p2, c = x.shape\n",
    "        \n",
    "        cls_tok = self.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tok, x), dim=1)\n",
    "        x = x + self.pos_emb # I think pos_emb is automatically broadcast in the batch dimension here.\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, emb_len, patch_num, layers=8, heads=4, dim_ff=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_len,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True # <-- This took me forever to find, default is to place batch in the second dimension\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=layers,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.transformer(x)\n",
    "        return y\n",
    "\n",
    "class ClassHead(nn.Module):\n",
    "    def __init__(self, emb_len, patch_num, hidden_sz, classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc0 = nn.Linear(emb_len, hidden_sz)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(hidden_sz, classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        x = inp[:, 0, :]\n",
    "        x = self.fc0(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class VIT(nn.Module):\n",
    "    def __init__(self, num_classes, patchsz, emb_dim, layers=4, heads=4, dim_ff=64, load=None):\n",
    "        super().__init__()\n",
    "        self.tokenizer = Tokenizer(3, emb_dim, patchsz)\n",
    "        self.embeddings = Embeddings(emb_dim, patch_num, 0.1) # 400 = (640 / 32)^2\n",
    "        self.transformer = Transformer(emb_dim, patch_num + 1, layers=layers, heads=heads, dim_ff=dim_ff)\n",
    "        self.class_head = ClassHead(emb_dim, patch_num + 1, 256, num_classes)\n",
    "        if load and os.path.isfile(load):\n",
    "            print(f\"Will load weights from {load}\")\n",
    "            self.load_state_dict(torch.load(load))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        x = self.embeddings(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.class_head(x)\n",
    "        return x\n",
    "\n",
    "    def save(self, name='vit_classification.pth'):\n",
    "        torch.save(self.state_dict(), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d610b-31f4-4bb3-a412-9015ab1ac43a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Expect ~72% accuracy on CIFAR10 with this config.\n",
    "imgsz = 32  # Images are this many pixels tall and wide in CIFAR10\n",
    "patchsz = 4 # Patchsize of 2 results in 16x16 patches. patchsz=4 seems fine too.\n",
    "emb_dim = 256  # AKA dmodel. Try reduce this if it takes too long to train.\n",
    "patch_num = (imgsz // patchsz)**2\n",
    "vit = VIT(\n",
    "    10,\n",
    "    patchsz,\n",
    "    emb_dim,\n",
    "    layers=4,\n",
    "    heads=4,\n",
    "    dim_ff=256,\n",
    "    load='vit_classification.pth'  # Set to the saved model path to start from a checkpoint.\n",
    ").to(device)\n",
    "\n",
    "epochs = 100\n",
    "optimizer = torch.optim.AdamW(vit.parameters(), lr=1e-4)\n",
    "lossfn = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_plot = []\n",
    "for epoch in range(epochs):\n",
    "    vit.train()\n",
    "    for i, (images, target) in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        images = images.float().to(device)\n",
    "        targets = target.to(device)\n",
    "\n",
    "        outs = vit(images)\n",
    "        loss = lossfn(outs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses = []\n",
    "    vit.eval()\n",
    "    correct = 0\n",
    "    total = len(cifar_eval)\n",
    "    for i, (images, target) in enumerate(tqdm(val_loader)):\n",
    "        with torch.no_grad():\n",
    "            images = images.float().to(device)\n",
    "            targets = target.to(device)\n",
    "            outs = vit(images)\n",
    "            loss = lossfn(outs, targets)\n",
    "            losses.append(loss)\n",
    "            for x in range(outs.shape[0]):\n",
    "                preds = F.softmax(outs, dim=1)\n",
    "                cls = preds[x].argmax()\n",
    "                lbl = targets[x]\n",
    "                if cls == lbl:\n",
    "                    correct += 1\n",
    "\n",
    "    epoch_loss = torch.Tensor(losses).mean().item()\n",
    "    print(\"Epoch {}, Current loss is {}\".format(epoch, epoch_loss))\n",
    "    print(\"{}/{} correct, {:.2f}%\".format(correct, total, 100*correct/total))\n",
    "    loss_plot.append(epoch_loss)\n",
    "\n",
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e9f54-c0e7-4fc9-a505-c16074ecbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn \n",
    "from sklearn import metrics\n",
    "\n",
    "evals = cifar_eval\n",
    "right = 0; total = 0; y_pred=[]; y_true=[]\n",
    "\n",
    "vit.eval()\n",
    "with torch.no_grad():\n",
    "    for image, target in tqdm(cifar_eval):\n",
    "        pred = vit(image.unsqueeze(0).float().to(device))\n",
    "        pred = F.softmax(pred, dim=1).argmax()\n",
    "        \n",
    "        y_pred.append(pred.item())\n",
    "        y_true.append(target)\n",
    "\n",
    "metrics.ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5478b9c-1d62-4cc3-91b8-37cffdc7f458",
   "metadata": {},
   "source": [
    "Here are your CIFAR10 classes for reference\n",
    "```\n",
    "0:\"Airplane\"\n",
    "1:\"Automobile\"\n",
    "2:\"Bird\"\n",
    "3:\"Cat\"\n",
    "4:\"Deer\"\n",
    "5:\"Dog\"\n",
    "6:\"Frog\"\n",
    "7:\"Horse\"\n",
    "8:\"Ship\"\n",
    "9:\"Truck\"\n",
    "```\n",
    "Your confusion matrix will likely show error between 3 and 5, cats and dogs, which seems reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934128bc-9e44-4212-9472-26d1c2419013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save your weights.\n",
    "# vit.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

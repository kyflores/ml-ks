{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2918e753-e36f-40f1-9513-6db57ebfcfee",
   "metadata": {},
   "source": [
    "# Vector search with sentence transformers and FAISS\n",
    "`conda install faiss-gpu sentence-transformers transformers datasets accelerate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2867d7ee-0d42-490a-a16e-ebcb93e03971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04a851-5204-4c2a-88f4-09efdf51a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename ='shakespeare.txt'\n",
    "if not os.path.exists(filename):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a9dc60-9c4c-49ba-9d86-1c7a17b45b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = text.split('\\n\\n')\n",
    "\n",
    "# Since all the lines look like CHARACTERNAME: Their lines, we want to cut off the character name so the model doesn't see that part.\n",
    "clean_samples = [x.split(\":\")[1].strip() for x in samples]\n",
    "num_samples = len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f2806-06a1-4d1f-b296-68d3d5c017ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2') # Use Dot product as your metric\n",
    "dmodel = model.get_sentence_embedding_dimension() # Match to the size of the model's embedding vector\n",
    "print(dmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5c51a7-12e6-40f9-b445-d19cfad63d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute embeddings for the entire dataset\n",
    "embeddings = [model.encode(sample, convert_to_numpy=True, device='cuda') for sample in tqdm(clean_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b91125-9bc3-4749-b0d0-a619a7d3d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# train_samples=num_samples // 2\n",
    "quantizer = faiss.IndexFlatL2(dmodel)\n",
    "\n",
    "# Set this to true to use an approximate, quantized index. Practically it's for massive datasets, which this is not.\n",
    "do_quantized=False\n",
    "nlist = int(math.sqrt(num_samples)) # Number of partitioning cells, 4 * sqrt(n) where n is the dataset size is a good heuristic apparently.\n",
    "\n",
    "# TODO: I think Inner Product is the closest to the sentence transformers dotproduct?\n",
    "# This index quantizes to reduce memory usage\n",
    "if do_quantized:\n",
    "    m=8 # Number of sub quantizers\n",
    "    bits_per_ix = 8\n",
    "    \n",
    "    index = faiss.IndexIVFPQ(quantizer, dmodel, nlist, m, bits_per_ix, faiss.METRIC_INNER_PRODUCT)\n",
    "else:\n",
    "    # This index uses full vectors and is more accurate and memory intensive\n",
    "    index = faiss.IndexIVFFlat(quantizer, dmodel, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "# Uncomment to use GPU index. \n",
    "# This can slow the first time it runs b/c CUDA kernels need to build.\n",
    "if torch.cuda.is_available():\n",
    "    gpu_r = faiss.StandardGpuResources()\n",
    "    index = faiss.index_cpu_to_gpu(gpu_r, 0, index)\n",
    "\n",
    "print(\"Training index...\")\n",
    "index.train(np.stack(embeddings))\n",
    "print(\"Index is trained?:\", index.is_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4f73b-8cf7-432d-bcbe-97d642f6b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add with ids lets you store another arbitrary vector with the vectors for retrieval\n",
    "# Useful if other metadata associated with the vectors must be retrieved from elsewhere.\n",
    "# In this case we're just pairing it with an index into the samples list so we can print it.\n",
    "index.add_with_ids(np.stack(embeddings), np.arange(len(embeddings)))\n",
    "print(\"Added\", index.ntotal, \"items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e3c78-fbe8-451d-87ea-760a625b1543",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.nprobe = 16 # Probe this many partitioning cells\n",
    "query = \"Look, he's winding up the watch of his wit\"\n",
    "query_emb = model.encode([query], convert_to_tensor=True)\n",
    "dists, idxs = index.search(query_emb.cpu().numpy(), 10)\n",
    "for d, x in zip(dists[0], idxs[0]):\n",
    "    print(f\"Score: {d}\")\n",
    "    print(samples[x])\n",
    "    print(\"=========================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
